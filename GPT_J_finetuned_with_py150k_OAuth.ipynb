{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df2b362-bb17-4532-9054-a103801e2cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeed\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from deepspeed.ops.transformer.inference import DeepSpeedTransformerInference\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44cc922a-9239-43cd-bff4-6dd44751d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check deepspeed installation\n",
    "report = !python3 -m deepspeed.env_report\n",
    "r = re.compile('.*ninja.*OKAY.*')\n",
    "assert any(r.match(line) for line in report) == True, \"DeepSpeed Inference not correct installed\"\n",
    "\n",
    "# check cuda and torch version\n",
    "torch_version, cuda_version = torch.__version__.split(\"+\")\n",
    "torch_version = \".\".join(torch_version.split(\".\")[:2])\n",
    "cuda_version = f\"{cuda_version[2:4]}.{cuda_version[4:]}\"\n",
    "r = re.compile(f'.*torch.*{torch_version}.*')\n",
    "assert any(r.match(line) for line in report) == True, \"Wrong Torch version\"\n",
    "r = re.compile(f'.*cuda.*{cuda_version}.*')\n",
    "assert any(r.match(line) for line in report) == True, \"Wrong Cuda version\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5122782d-1bcc-4bc1-81b5-7d22bc34144b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is loaded on device cpu\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('/data/kiho/autocode/GPTJ/Finetune_GPTNEO_GPTJ6B/finetuning_repo/finetuned/checkpoint-6/')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/data/kiho/autocode/GPTJ/Finetune_GPTNEO_GPTJ6B/finetuning_repo/finetuned/checkpoint-6/')\n",
    "# device = torch.device('cuda')\n",
    "# model.to(device)\n",
    "print(f'model is loaded on device {model.device.type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93c64752-3220-45a2-9b46-de70430ec8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency(model, tokenizer, payload, generation_args={},device=model.device):\n",
    "    input_ids = tokenizer(payload, return_tensors=\"pt\").input_ids.to(device)\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(2):\n",
    "        _ =  model.generate(input_ids, **generation_args)\n",
    "    # Timed run\n",
    "    for _ in range(10):\n",
    "        start_time = perf_counter()\n",
    "        _ = model.generate(input_ids, **generation_args)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return f\"latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_p95_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c99f6-5b68-44d9-823b-fe0145868de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained('/data/kiho/autocode/GPTJ/Finetune_GPTNEO_GPTJ6B/finetuning_repo/finetuned/checkpoint-6/')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('/data/kiho/autocode/GPTJ/Finetune_GPTNEO_GPTJ6B/finetuning_repo/finetuned/checkpoint-6/')\n",
    "# # device = torch.device('cuda:3')\n",
    "# # model.to(device)\n",
    "# # print(f'model is loaded on device {model.device.type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e673eef-16b2-4d4d-91b4-3be1e6277c2b",
   "metadata": {},
   "source": [
    "##### Normal text inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bdcf7-1631-4c83-8e22-4f92bc9b9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload = \"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend but it\"\n",
    "\n",
    "# input_ids = tokenizer(payload,return_tensors=\"pt\").input_ids.to(model.device)\n",
    "# print(f\"input payload: \\n \\n{payload}\")\n",
    "# logits = model.generate(input_ids, do_sample=True, num_beams=1, min_length=128, max_new_tokens=128)\n",
    "\n",
    "# print(f\"prediction: \\n \\n {tokenizer.decode(logits[0].tolist()[len(input_ids[0]):])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ffa266-755d-44ba-8459-b6ea3ff755ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload=\"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend but it\"*2\n",
    "# print(f'Payload sequence length is: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "# # generation arguments\n",
    "# generation_args = dict(\n",
    "#   do_sample=False,\n",
    "#   num_beams=1,\n",
    "#   min_length=128,\n",
    "#   max_new_tokens=128\n",
    "# )\n",
    "# vanilla_results = measure_latency(model,tokenizer,payload,generation_args)\n",
    "\n",
    "# print(f\"Vanilla model: {vanilla_results[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda84a7d-53f0-46b6-ba70-5785380187d7",
   "metadata": {},
   "source": [
    "##### DeepSpeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b620dc4e-dcbe-4388-8b56-90e873c6db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-14 11:22:12,847] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed info: version=0.7.0, git-hash=unknown, git-branch=unknown\n",
      "[2023-02-14 11:22:12,848] [INFO] [logging.py:68:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Installed CUDA version 11.0 does not match the version torch was compiled with 11.3 but since the APIs are compatible, accepting this combination\n",
      "Using /home/kiho/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/kiho/.cache/torch_extensions/py39_cu113/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 2.8810627460479736 seconds\n",
      "[2023-02-14 11:22:20,959] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 4096, 'intermediate_size': 16384, 'heads': 16, 'num_hidden_layers': -1, 'fp16': True, 'pre_layer_norm': True, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'q_int8': False, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': 64, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': False, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False}\n",
      "model is loaded on device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# init deepspeed inference engine\n",
    "ds_model = deepspeed.init_inference(\n",
    "    model=model,      # Transformers models\n",
    "    mp_size=1,        # Number of GPU\n",
    "    dtype=torch.float16, # dtype of the weights (fp16)\n",
    "    replace_method=\"auto\", # Lets DS autmatically identify the layer to replace\n",
    "    replace_with_kernel_inject=True, # replace the model with the kernel injector\n",
    ")\n",
    "# ds_model.to(model.device)\n",
    "print(f\"model is loaded on device {ds_model.module.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea843546-b44e-4775-b321-4860ca8cd979",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(ds_model.module.transformer.h[0], DeepSpeedTransformerInference) == True, \"Module not sucessfully initialized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5818cf4-f571-44fb-a236-587e7d1acefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /twitter/login HTTP/1.1\n",
      "Host: api.twitter.com\n",
      "User-Agent: Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm+ Bing/2.0)\n",
      "Accept: */*\n",
      "Proxy-Connection: Keep-Alive\n",
      "X-UA-Compatible: IE=Edge,chrome=1\n",
      "Cookie: a=s%3D%253\n"
     ]
    }
   ],
   "source": [
    "example = \"GET /twitter/login HTTP/1.1\"\n",
    "input_ids = tokenizer(example,return_tensors=\"pt\").input_ids.to(ds_model.module.device)\n",
    "logits = ds_model.generate(input_ids, do_sample=True, top_p=0.95, max_length=100)\n",
    "print(tokenizer.decode(logits[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cb957ce-c10d-4558-a2ff-a6082d57c9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://auth-server.com/auth?state=9&code=123-456-789-0123-4567-89012-3456789-0123456789&grant_type=authorization_code&redirect_uri=http://redirect.redirect-uri.com.\n",
      "\n",
      "With this setup and all in order we should be able to log in with:\n",
      "\n",
      "http://www.example.com/auth/authorize\n"
     ]
    }
   ],
   "source": [
    "example = \"https://auth-server.com/auth?\"\n",
    "input_ids = tokenizer(example,return_tensors=\"pt\").input_ids.to(ds_model.module.device)\n",
    "logits = ds_model.generate(input_ids, do_sample=True, top_p=0.95, max_length=100)\n",
    "print(tokenizer.decode(logits[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76f92226-ce74-4716-b5e3-644db9ad7392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sequence length is: 13\n",
      "DeepSpeed model: latency (ms) - 3316.9458935037255; Average latency (ms) - 3308.97 +\\- 7.42;\n"
     ]
    }
   ],
   "source": [
    "payload = (\n",
    "    \"# Import re and define a regular expression that matches an email address\"\n",
    ")\n",
    "print(f'Payload sequence length is: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "# generation arguments\n",
    "generation_args = dict(do_sample=False, num_beams=1, min_length=30, max_new_tokens=128)\n",
    "ds_results = measure_latency(ds_model, tokenizer, payload, generation_args, ds_model.module.device)\n",
    "\n",
    "print(f\"DeepSpeed model: {ds_results[0]}\")\n",
    "# Payload sequence length is: 128\n",
    "# DeepSpeed model: P95 latency (ms) - 6577.044982599967; Average latency (ms) - 6569.11 +\\- 6.57;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df2b362-bb17-4532-9054-a103801e2cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeed\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from deepspeed.ops.transformer.inference import DeepSpeedTransformerInference\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44cc922a-9239-43cd-bff4-6dd44751d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check deepspeed installation\n",
    "report = !python3 -m deepspeed.env_report\n",
    "r = re.compile('.*ninja.*OKAY.*')\n",
    "assert any(r.match(line) for line in report) == True, \"DeepSpeed Inference not correct installed\"\n",
    "\n",
    "# check cuda and torch version\n",
    "torch_version, cuda_version = torch.__version__.split(\"+\")\n",
    "torch_version = \".\".join(torch_version.split(\".\")[:2])\n",
    "cuda_version = f\"{cuda_version[2:4]}.{cuda_version[4:]}\"\n",
    "r = re.compile(f'.*torch.*{torch_version}.*')\n",
    "assert any(r.match(line) for line in report) == True, \"Wrong Torch version\"\n",
    "r = re.compile(f'.*cuda.*{cuda_version}.*')\n",
    "assert any(r.match(line) for line in report) == True, \"Wrong Cuda version\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5122782d-1bcc-4bc1-81b5-7d22bc34144b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is loaded on device cpu\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('/data/kiho/autocode/GPTJ/Finetune_GPTNEO_GPTJ6B/finetuning_repo/finetuned/checkpoint-6/')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/data/kiho/autocode/GPTJ/Finetune_GPTNEO_GPTJ6B/finetuning_repo/finetuned/checkpoint-6/')\n",
    "# device = torch.device('cuda')\n",
    "# model.to(device)\n",
    "print(f'model is loaded on device {model.device.type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93c64752-3220-45a2-9b46-de70430ec8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency(model, tokenizer, payload, generation_args={},device=model.device):\n",
    "    input_ids = tokenizer(payload, return_tensors=\"pt\").input_ids.to(device)\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(2):\n",
    "        _ =  model.generate(input_ids, **generation_args)\n",
    "    # Timed run\n",
    "    for _ in range(10):\n",
    "        start_time = perf_counter()\n",
    "        _ = model.generate(input_ids, **generation_args)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return f\"latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_p95_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c99f6-5b68-44d9-823b-fe0145868de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained('/data/kiho/autocode/GPTJ/Finetune_GPTNEO_GPTJ6B/finetuning_repo/finetuned/checkpoint-6/')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('/data/kiho/autocode/GPTJ/Finetune_GPTNEO_GPTJ6B/finetuning_repo/finetuned/checkpoint-6/')\n",
    "# # device = torch.device('cuda:3')\n",
    "# # model.to(device)\n",
    "# # print(f'model is loaded on device {model.device.type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e673eef-16b2-4d4d-91b4-3be1e6277c2b",
   "metadata": {},
   "source": [
    "##### Normal text inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bdcf7-1631-4c83-8e22-4f92bc9b9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload = \"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend but it\"\n",
    "\n",
    "# input_ids = tokenizer(payload,return_tensors=\"pt\").input_ids.to(model.device)\n",
    "# print(f\"input payload: \\n \\n{payload}\")\n",
    "# logits = model.generate(input_ids, do_sample=True, num_beams=1, min_length=128, max_new_tokens=128)\n",
    "\n",
    "# print(f\"prediction: \\n \\n {tokenizer.decode(logits[0].tolist()[len(input_ids[0]):])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ffa266-755d-44ba-8459-b6ea3ff755ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload=\"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend but it\"*2\n",
    "# print(f'Payload sequence length is: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "# # generation arguments\n",
    "# generation_args = dict(\n",
    "#   do_sample=False,\n",
    "#   num_beams=1,\n",
    "#   min_length=128,\n",
    "#   max_new_tokens=128\n",
    "# )\n",
    "# vanilla_results = measure_latency(model,tokenizer,payload,generation_args)\n",
    "\n",
    "# print(f\"Vanilla model: {vanilla_results[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda84a7d-53f0-46b6-ba70-5785380187d7",
   "metadata": {},
   "source": [
    "##### DeepSpeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b620dc4e-dcbe-4388-8b56-90e873c6db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-10 17:25:32,721] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.0, git-hash=unknown, git-branch=unknown\n",
      "[2023-02-10 17:25:32,723] [INFO] [logging.py:68:log_dist] [Rank 0] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Installed CUDA version 11.0 does not match the version torch was compiled with 11.3 but since the APIs are compatible, accepting this combination\n",
      "Using /home/kiho/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...\n",
      "Creating extension directory /home/kiho/.cache/torch_extensions/py39_cu113/transformer_inference...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/kiho/.cache/torch_extensions/py39_cu113/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/8] /data/kiho/cuda-11.0/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/TH -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/THC -isystem /data/kiho/cuda-11.0/include -isystem /data/kiho/mambaforge/envs/autocomplete/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/dequantize.cu -o dequantize.cuda.o \n",
      "[2/8] /data/kiho/cuda-11.0/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/TH -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/THC -isystem /data/kiho/cuda-11.0/include -isystem /data/kiho/mambaforge/envs/autocomplete/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/softmax.cu -o softmax.cuda.o \n",
      "/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/softmax.cu(271): warning: variable \"alibi_offset\" was declared but never referenced\n",
      "\n",
      "[3/8] /data/kiho/cuda-11.0/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/TH -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/THC -isystem /data/kiho/cuda-11.0/include -isystem /data/kiho/mambaforge/envs/autocomplete/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/normalize.cu -o normalize.cuda.o \n",
      "/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/normalize.cu(23): warning: variable \"iterations\" was declared but never referenced\n",
      "\n",
      "/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/normalize.cu(93): warning: variable \"iterations\" was declared but never referenced\n",
      "\n",
      "/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/normalize.cu(310): warning: variable \"res_add_cast\" was declared but never referenced\n",
      "\n",
      "[4/8] /data/kiho/cuda-11.0/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/TH -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/THC -isystem /data/kiho/cuda-11.0/include -isystem /data/kiho/mambaforge/envs/autocomplete/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu -o apply_rotary_pos_emb.cuda.o \n",
      "[5/8] /data/kiho/cuda-11.0/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/TH -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/THC -isystem /data/kiho/cuda-11.0/include -isystem /data/kiho/mambaforge/envs/autocomplete/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu -o transform.cuda.o \n",
      "/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(27): warning: variable \"d0_out_stride\" was declared but never referenced\n",
      "\n",
      "/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(28): warning: variable \"d1_out_stride\" was declared but never referenced\n",
      "\n",
      "/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(54): warning: variable \"lane\" was declared but never referenced\n",
      "\n",
      "/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(92): warning: variable \"half_dim\" was declared but never referenced\n",
      "\n",
      "/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(107): warning: variable \"vals_half\" was declared but never referenced\n",
      "\n",
      "/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(108): warning: variable \"output_half\" was declared but never referenced\n",
      "\n",
      "/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(125): warning: variable \"lane\" was declared but never referenced\n",
      "\n",
      "[6/8] /data/kiho/cuda-11.0/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/TH -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/THC -isystem /data/kiho/cuda-11.0/include -isystem /data/kiho/mambaforge/envs/autocomplete/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/gelu.cu -o gelu.cuda.o \n",
      "/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/gelu.cu(490): warning: variable \"coef_cast2\" was declared but never referenced\n",
      "\n",
      "[7/8] c++ -MMD -MF pt_binding.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/TH -isystem /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/include/THC -isystem /data/kiho/cuda-11.0/include -isystem /data/kiho/mambaforge/envs/autocomplete/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp -o pt_binding.o \n",
      "[8/8] c++ pt_binding.o gelu.cuda.o normalize.cuda.o softmax.cuda.o dequantize.cuda.o apply_rotary_pos_emb.cuda.o transform.cuda.o -shared -lcurand -L/data/kiho/mambaforge/envs/autocomplete/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/data/kiho/cuda-11.0/lib64 -lcudart -o transformer_inference.so\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 31.746678352355957 seconds\n",
      "[2023-02-10 17:26:06,385] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 4096, 'intermediate_size': 16384, 'heads': 16, 'num_hidden_layers': -1, 'fp16': True, 'pre_layer_norm': True, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'q_int8': False, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': 64, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': False, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False}\n",
      "model is loaded on device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# init deepspeed inference engine\n",
    "ds_model = deepspeed.init_inference(\n",
    "    model=model,      # Transformers models\n",
    "    mp_size=1,        # Number of GPU\n",
    "    dtype=torch.float16, # dtype of the weights (fp16)\n",
    "    replace_method=\"auto\", # Lets DS autmatically identify the layer to replace\n",
    "    replace_with_kernel_inject=True, # replace the model with the kernel injector\n",
    ")\n",
    "# ds_model.to(model.device)\n",
    "print(f\"model is loaded on device {ds_model.module.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea843546-b44e-4775-b321-4860ca8cd979",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(ds_model.module.transformer.h[0], DeepSpeedTransformerInference) == True, \"Module not sucessfully initialized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5818cf4-f571-44fb-a236-587e7d1acefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Import re and define a regular expression that matches an email address.\n",
      "import re\n",
      "email_re = re.compile(r'''\n",
      "    ^[a-z0-9._%-]+@([a-z0-9.-]+\\.)+[a-z]{2,4}$\n",
      "''', re.VERBOSE)\n",
      "\n",
      "Now, suppose I have a string:\n",
      "\"this@is@an@email.address.\n"
     ]
    }
   ],
   "source": [
    "example = \"# Import re and define a regular expression that matches an email address\"\n",
    "input_ids = tokenizer(example,return_tensors=\"pt\").input_ids.to(ds_model.module.device)\n",
    "logits = ds_model.generate(input_ids, do_sample=True, top_p=0.95, max_length=100)\n",
    "print(tokenizer.decode(logits[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f92226-ce74-4716-b5e3-644db9ad7392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sequence length is: 13\n"
     ]
    }
   ],
   "source": [
    "payload = (\n",
    "    \"# Import re and define a regular expression that matches an email address\"\n",
    ")\n",
    "print(f'Payload sequence length is: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "# generation arguments\n",
    "generation_args = dict(do_sample=False, num_beams=1, min_length=30, max_new_tokens=128)\n",
    "ds_results = measure_latency(ds_model, tokenizer, payload, generation_args, ds_model.module.device)\n",
    "\n",
    "print(f\"DeepSpeed model: {ds_results[0]}\")\n",
    "# Payload sequence length is: 128\n",
    "# DeepSpeed model: P95 latency (ms) - 6577.044982599967; Average latency (ms) - 6569.11 +\\- 6.57;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
